# üöÄ Guia de Instala√ß√£o - Ollama (IA Gratuita e Local)

## O que √© Ollama?

Ollama permite rodar modelos de IA **localmente** no seu computador, **sem custo** e **sem precisar de API keys**. √â 100% gratuito e open source!

## üì• Passo 1: Instalar o Ollama

### Windows

1. Baixe o instalador: https://ollama.com/download/windows
2. Execute o arquivo `.exe` baixado
3. Siga o assistente de instala√ß√£o

### Linux

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### macOS

1. Baixe o aplicativo: https://ollama.com/download/mac
2. Arraste para a pasta Aplicativos
3. Execute o Ollama

## ü§ñ Passo 2: Baixar um Modelo de IA

Ap√≥s instalar, abra o terminal/PowerShell e execute:

```bash
# Modelo recomendado (r√°pido e eficiente - ~2GB)
ollama pull llama3.2

# Alternativas:
ollama pull mistral        # √ìtimo para portugu√™s
ollama pull gemma          # Desenvolvido pelo Google
ollama pull phi3           # Pequeno e r√°pido
```

## ‚ñ∂Ô∏è Passo 3: Iniciar o Ollama

```bash
ollama serve
```

Deixe este terminal aberto enquanto usar a aplica√ß√£o.

## üß™ Passo 4: Testar

Em outro terminal:

```bash
ollama run llama3.2
```

Digite uma mensagem para testar. Para sair: `/bye`

## üîß Configura√ß√£o no Projeto

1. No seu arquivo `.env` (copie de `.env.example`):

```
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
```

2. Pronto! Agora execute o backend:

```bash
cd backend
python app.py
```

## üìä Modelos Dispon√≠veis

| Modelo   | Tamanho | Velocidade | Qualidade | Recomendado para |
| -------- | ------- | ---------- | --------- | ---------------- |
| llama3.2 | ~2GB    | ‚ö°‚ö°‚ö°     | ‚≠ê‚≠ê‚≠ê    | Uso geral        |
| mistral  | ~4GB    | ‚ö°‚ö°       | ‚≠ê‚≠ê‚≠ê‚≠ê  | Portugu√™s        |
| llama3   | ~5GB    | ‚ö°‚ö°       | ‚≠ê‚≠ê‚≠ê‚≠ê  | Alta qualidade   |
| gemma    | ~5GB    | ‚ö°‚ö°       | ‚≠ê‚≠ê‚≠ê‚≠ê  | Balanceado       |
| phi3     | ~2GB    | ‚ö°‚ö°‚ö°     | ‚≠ê‚≠ê‚≠ê    | PC mais fracos   |

## üí° Dicas

- **Primeira execu√ß√£o**: O modelo demora um pouco para carregar (normal)
- **Performance**: Quanto mais RAM, melhor
- **GPU**: Se tiver GPU NVIDIA, Ollama usa automaticamente
- **Sem internet**: Funciona 100% offline depois de baixar o modelo

## üÜò Resolu√ß√£o de Problemas

### "Ollama n√£o est√° rodando"

```bash
ollama serve
```

### Trocar de modelo

No `.env`, mude:

```
OLLAMA_MODEL=mistral
```

### Ver modelos instalados

```bash
ollama list
```

### Remover modelo

```bash
ollama rm nome-do-modelo
```

## üåê Recursos

- Site oficial: https://ollama.com
- Modelos dispon√≠veis: https://ollama.com/library
- Documenta√ß√£o: https://github.com/ollama/ollama

---

‚ú® **Vantagens do Ollama:**

- ‚úÖ 100% Gratuito
- ‚úÖ Sem limites de uso
- ‚úÖ Privacidade total (seus dados n√£o saem do PC)
- ‚úÖ Funciona offline
- ‚úÖ Sem API keys necess√°rias
