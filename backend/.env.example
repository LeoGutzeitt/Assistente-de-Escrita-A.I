# Copie este arquivo para .env e configure (opcional)

# Configurações do Ollama (local - gratuito)
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2

# Modelos disponíveis: llama3.2, llama3, mistral, gemma, phi3, etc.
# Para baixar um modelo: ollama pull llama3.2
